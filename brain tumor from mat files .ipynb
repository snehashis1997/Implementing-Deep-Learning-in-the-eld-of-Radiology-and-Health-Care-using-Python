{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nseed = 42\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport cv2\nimport copy\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,BatchNormalization,GaussianNoise\nfrom keras.layers import MaxPool2D,ZeroPadding2D,MaxPooling2D\nfrom keras.layers import Flatten,Activation\nfrom keras.layers import Dense,Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam,SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras import initializers\nimport numpy as np\nfrom keras import regularizers\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import auc,roc_curve\n\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom glob import glob\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=112\n\n\nbatchsize =16\n\n#data_path id\ndata_path=r\"../input/tumor/tumor/image/image/id_\"\n\npngs=[]\n\n#pngs=glob(data_path)\n\nfor i in range(1,3065):\n    pngs.append(data_path+str(i)+'.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n#the csv label id path\n\ndf=pd.read_csv(\"../input/tumor/tumor/mat2python_mri_tumor.csv\")\n\ny_true=[]\n\nfor i in range(3064):\n    \n    y_true.append(int(df['CLASS NO'][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(y_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(y_true)\ny_true = encoder.transform(y_true)\ny_true = to_categorical(y_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set(y_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1./255,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1./ 255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta = 0, \n                          patience = 2,\n                          verbose = 1,\n                          restore_best_weights = True)\n\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n\n# we put our call backs into a callback list\ncallbacks = [earlystop, reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.layers import Input,GlobalAveragePooling2D\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = VGG16(weights='imagenet',input_shape = (target, target, 3), include_top=False)\n\nfor (i,layer) in enumerate(base_model.layers):\n    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)\nx = base_model.output\nx = Flatten(name = \"flatten\")(x)# let's add a fully-connected layer\nx = Dense(512,kernel_regularizer=regularizers.l2(0.00001),activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\n\nx = Dense(256,kernel_regularizer=regularizers.l2(0.00001),activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\n\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(3, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in base_model.layers:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor (i,layer) in enumerate(base_model.layers):\n    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (i,layer) in enumerate(model.layers):\n    \n    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=list()\n\nfor i in range(len(pngs)):\n    \n    img=cv2.imread(pngs[i])\n    #img=cv2.equalizeHist(img)\n    img=cv2.resize(img,(target,target))\n    dataset.append(img)\n\ndataset=np.array(dataset)\n\nlen(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trainset,X_test ,y_trainset,y_test= train_test_split(dataset,y_true,test_size=0.2, shuffle=True,random_state=seed)\n\nX_trainset = X_trainset.reshape(-1,target,target,3)\nX_test = X_test.reshape(-1,target,target,3)\n\n\nX_trainset.shape,len(y_trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test/np.max(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test=test_datagen.flow(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_valid,y_train,y_valid = train_test_split(X_trainset,y_trainset,test_size=0.3, shuffle=False,random_state=seed)\n\nX_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['acc'])\n\noutput =model.fit_generator(train_datagen.flow(X_train, y_train, seed=seed,batch_size=batchsize),steps_per_epoch=X_train.shape[0]//batchsize, \n                            epochs=20, verbose=1,callbacks=callbacks,\n                            validation_data=test_datagen.flow(X_valid,y_valid,batch_size=batchsize,seed=seed),\n                            validation_steps=X_valid.shape[0]//batchsize,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(output.history['acc'])\nplt.plot(output.history['val_acc'])\nplt.title('classifier accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(output.history['loss'])\nplt.plot(output.history['val_loss'])\nplt.title('classifier loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (i,layer) in enumerate(model.layers):\n    \n    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N=15\n\nfor layer in model.layers[:N]:\n   layer.trainable = False\nfor layer in model.layers[N:]:\n   layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (i,layer) in enumerate(model.layers):\n    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(0.00001), loss='categorical_crossentropy', metrics=['acc'])\n\noutput =model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batchsize),steps_per_epoch=X_train.shape[0]//batchsize, \n                                 epochs=20, verbose=1,callbacks=callbacks,\n                                 validation_data=test_datagen.flow(X_valid,y_valid,batch_size=batchsize), validation_steps=X_valid.shape[0]//batchsize,shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(output.history['acc'])\nplt.plot(output.history['val_acc'])\nplt.title('classifier accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(output.history['loss'])\nplt.plot(output.history['val_loss'])\nplt.title('classifier loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=model.evaluate_generator(test_datagen.flow(X_test, y_test, batch_size=batchsize),steps=y_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test accuracy is: ' + str(score[1]))\n\nprint('test loss is: '+ str(score[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape[0]//batchsize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model.predict(X_test, batch_size=2,verbose=1, steps=None)\ny_hat_class = y_hat.argmax(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(y_hat_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_class = y_test.argmax(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test_class,y_hat_class)\n\nreport=classification_report(y_test_class,y_hat_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(confusion_matrix):\n    diagonal_sum = confusion_matrix.trace()\n    sum_of_all_elements = confusion_matrix.sum()\n    return diagonal_sum / sum_of_all_elements ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cm)\nprint('\\n')\n\nprint(report)\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy=(cm[0][0] + cm[1][1] + cm[2][2])*100//y_hat.shape[0]\n\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('best.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}